{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of Central Limit Theorem\n",
    "Michelle Yaochai | Zheng Chong, Emily | Marvin Dragon Choo | Brian Ku\\\n",
    "01 Apr 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "We explore the Central Limit Theorem (CLT) and their applications in this notebook.\n",
    "\n",
    "* In Part A, we define and illustrate CLT using examples such as coin flips and rolling a fair dice. We also show how to use CLT to calculate population parameters from samples, as well as situations where CLT fails.\n",
    "* In Part B, we demonstrate the utility of CLT by simulating various physical systems, and validating key parameters against those predicted by CLT. We will be exploring the random walk in relation to CLT and other interesting applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following modules will be used in this notebook, each of which will be introduced as they are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Illustration\n",
    "\n",
    "### Motivation\n",
    "\n",
    "The Central Limit Theorem is one of the most fundamental theorems in the field of statistics, and it has important and wide-ranging application to other fields. We often find ourselves dealing with large numbers of variables.\n",
    "\n",
    "For example, within the population of NUS students, we might want to know the distribution of the number of hours of sleep per day. Instead of collecting the data from every single NUS student (*census*), we like to, statistically-speaking, collect *samples*. Ideally, we would do probability sampling and collect random samples of about 30 students. \n",
    "\n",
    "For the first sample of 30 students, their \"number of hours of sleep\" distribution might look something like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sample of 30 students sleep time\n",
    "y = [np.random.randint(0,11) for _ in range(30)]\n",
    "\n",
    "plt.hist(y,bins=11)\n",
    "plt.xlabel('Number of hours of sleep')\n",
    "plt.ylabel('Number of students')\n",
    "plt.title('Sample of 30 students')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we had many volunteers, each independently collecting a sample from some fixed number ($n$) of students. We can take the mean number of hours from each sample, and plot them in a histogram. We see that as $n$ increases, the **distribution of sample means will begin to look more and more like a normal curve**.\n",
    "\n",
    "This is the idea known as the Central Limit Theorem - which will be extremely useful as we can use properties of the Normal distribution to predict population parameters. In the example above, we would be able to estimate the mean number of hours of sleep for the population of NUS students by approximating the sampling distribution to a Normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "Here, we give a common definition of Central Limit Theorem (CLT):\n",
    "\n",
    "> The distribution of sample means approximates a normal distribution as the sample size gets larger, even if the population is not normally distributed.\n",
    "\n",
    "This definition is certainly not the most mathematically rigorous, but is sufficient for the scope of the project. There are also additional restrictions necessary for CLT, and they will be introduced as appropriate.\n",
    "\n",
    "We now illustrate the assertion that CLT is trying to make using coin flips. Consider the act of flipping a fair coin.\n",
    "* Since the coin is fair, there is an equal probability $p=0.5$ of it landing on heads or tails.\n",
    "* Assign the outcome of tails as 0, and heads as 1.\n",
    "\n",
    "This system is equivalently represented by a binomial distribution (since there are only two outcomes) with probability of success (of getting heads) of 0.5.\n",
    "\n",
    "To begin, let's see what happens when we flip the coin 100 times. Since this is a binomial distribution, we can make use of ```numpy.random.binomial``` to simulate these coin flips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "p = 0.5\n",
    "\n",
    "# Labels\n",
    "flip_outcomes = ['Tail', 'Head']\n",
    "\n",
    "# Data\n",
    "# Count the number of heads after flipping a coin n times, each with probability p of landing on heads.\n",
    "num_heads = np.random.binomial(n=n,p=p)\n",
    "# \n",
    "num_tails = n - num_heads\n",
    "\n",
    "# Show outcome\n",
    "print(f'Out of {n} coin flips, there were {num_heads} heads and {num_tails} tails.')\n",
    "\n",
    "# Plot\n",
    "# Expected\n",
    "plt.bar(flip_outcomes, [num_tails/n, num_heads/n], width=0.25, label='Actual')\n",
    "# Actual\n",
    "plt.hlines(y=0.5, xmin=-0.6, xmax=1.6, linestyles='--',\n",
    "           colors='red', label='Expected')\n",
    "\n",
    "# Customize and Show\n",
    "plt.xlim([-0.5, 1.5])\n",
    "plt.title(f'Outcome of {n} dice flips')\n",
    "plt.legend(loc='lower center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If asked to predict the number of heads, it is probably *instinctive* to expect 50 of each, after all, we expect an equal number of heads and tails.\n",
    "\n",
    "Most of the time, we get a number that *feels* close to, but not exactly 50. Try running the above cell multiple times! Here, we take this opportunity to explain more key terms.\n",
    "\n",
    "* Each time you execute the cell, you generate one **sample** of 100 coin flips.\n",
    "* The **sample size $n$** is the number of independent coin flips that were performed. Here, it is 100.\n",
    "* The direct output of each sample, which is a number between 0 and 100, is the total number of heads in that sample.\n",
    "\n",
    "### Sampling Distribution\n",
    "Now that you've run the cell multiple times, you can see that each time you run it, you get a different result. Even though the coin is the same, there is still some variability between the different samples collected from this 'population'.\n",
    "\n",
    "Yet, it is not equally likely to obtain each outcome. Using a sample size of 100, it *feels* more probable to observe a sample with 40 heads, than that of 10 heads, which feels more extreme. We can visualize this probability by simulating many independent samples, and observing the distribution of the number of heads, using a histogram.\n",
    "* This is what *'distribution of sample means'* in the definition of CLT is referring to.\n",
    "\n",
    "Surprisingly, generating this data can be done in one line of code, again using ```numpy.random.binomial```. In particular, we vary the sample sizes and observe the impact on the subsequent distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual sample sizes to test\n",
    "n = [3, 15, 30, 50]\n",
    "# Fair coin, equal chance of heads and tails\n",
    "p = 0.5\n",
    "# Generate 2000 independent samples\n",
    "s = 200\n",
    "\n",
    "# Data\n",
    "# Access via dist = arr[sample_size_index], dist = [mean1, mean2, ... mean2000]\n",
    "flip_distribution = [np.random.binomial(i, p, s) for i in n]\n",
    "\n",
    "# We will reuse this later, so putting it in a function first\n",
    "\n",
    "\n",
    "def plot_coin_flip_CLT(flip_distribution, n, normalize = False):\n",
    "    fig, ax = plt.subplots(ncols=len(n), figsize=(3*len(n), 4))\n",
    "\n",
    "    # Making good histograms\n",
    "    for i, dist in enumerate(flip_distribution):\n",
    "        min_value = min(dist)\n",
    "        max_value = max(dist)\n",
    "\n",
    "        # Max ensures that step sizes in the following histogram are at least 1, but otherwise such that there will be around 30 bins\n",
    "        step_size = max((max_value-min_value)//30, 1)\n",
    "        # n+2 ensures the outcome with all heads is also captured.\n",
    "        bin_range = range(0, n[i]+2, step_size)\n",
    "\n",
    "        # Plot histogram and customize\n",
    "        ax[i].hist(dist, bins=bin_range, density=normalize)\n",
    "        ax[i].set_xlabel('Heads')\n",
    "        ax[i].set_title(f'$n$ = {n[i]}')\n",
    "    # Common y axis label\n",
    "    if normalize:\n",
    "        ax[0].set_ylabel('Counts')\n",
    "    else:\n",
    "        ax[0].set_ylabel('Relative frequency')\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "plot_coin_flip_CLT(flip_distribution, n)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restating CLT from above, we can quite clearly observe that:\n",
    "> The *distribution of sample means* approximates a normal distribution *as the sample size gets larger*, even if the population is not normally distributed.\n",
    "\n",
    "* In particular, the 'distribution of population' here is the distribution of the probability of the outcomes of coin flip - 0.5 for heads and tails, certainly not normally distributed.\n",
    "\n",
    "### Additional Conditions\n",
    "In practice, we impose some further conditions before using CLT, to account for more extreme probability distribution functions. We will summarise the conditions necessary for CLT to hold true:\n",
    "\n",
    "* The data must be sampled randomly from the population by using a probability sampling method.\n",
    "* Each of the sample values must be independent of each other. That is, the occurrence of one event does not affect the occurrence of any other event.\n",
    "* When the sample is drawn without replacement, the sample size must be no larger than 10% of the total population.\n",
    "* Sample sizes equal to or greater than 30 are often considered sufficient for the CLT to hold.\n",
    "\n",
    "If all these conditions are fulfilled, then, by the central limit theorem, the sampling distribution obtained will represent a normal distribution, regardless whether the variable actually follows a normal distribution in the population. (eg. In Asia, people's heights are skewed towards the shorter end.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Application\n",
    "We can now easily control the conditions of sampling to generate a normal distribution. To turn CLT into a useful tool, we need to establish a link between the sample statistics and population statistics.\n",
    "\n",
    "#### Population statistics\n",
    "The mean and variance of a single flip of a fair coin is respectively given by:\n",
    "$$\\mu = 0.5\\cdot0 + 0.5\\cdot1 = 0.5$$\n",
    "$$\\sigma^2 = 0.5\\cdot(1-0.5)^2 + 0.5\\cdot(0-0.5)^2 = 0.25$$\n",
    "\n",
    "#### Expectation value\n",
    "Scaling this up to $n$ independent coin flips, we *expect* the subseuqent distribution to have both mean and variance scaled up by $n$.\n",
    "* *The mathematical derivation is beyond the scope of this project.*\n",
    "\n",
    "Let $X$ represent the distribution of number of heads (i.e. what we have been plotting), then,\n",
    "$$E(\\bar{X}) = n\\mu$$\n",
    "where $\\bar{X}$ is mean number of heads, and $\\mu$ is the expected number of heads for a flip.\n",
    "$$E(Var(X))=n\\sigma^2$$\n",
    "where $Var(X)$ is the variance of the number of heads, and $\\sigma^2$ is the expected variance in the number of heads per flip.\n",
    "\n",
    "#### Utility\n",
    "This also means that if we did not know the population mean and variance (for example, if we **did not know** that the coin is fair, and this is true in the applications of CLT), our best guess for the mean and variance would then be\n",
    "$$\\mu=\\bar{X}/n$$\n",
    "$$\\sigma^2 = Var(X)/n$$\n",
    "\n",
    "Therefore, in the following section we will do the following:\n",
    " \n",
    "1. Compare the **expected** distribution (normal distribution) against the actual data, and show a good agreement for high $n$ values.\n",
    "   * The expected distribution is normal, with $(\\mu,\\sigma^2)=(0.5n,0.25n)$\n",
    "   * In a normal curve, a higher mean corresponds to a curve shifted to right along the x-axis, and a higher variance corresponds to a wider curve.\n",
    "   * We use ```scipy.stats.norm``` to obtain this normal curve.\n",
    "2. Estimate the true mean and variance of the coin from the observed distributions.\n",
    "\n",
    "Note that we have also normalized the graph so that the y-axis now represents the relative frequency, which corresponds to the relative probability of getting a sample mean within the specified bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normal_points(mean=0,var=1,x_min=0,x_max=1):\n",
    "    x = np.linspace(x_min,x_max,endpoint=True)\n",
    "    # norm takes in mean,std\n",
    "    y = norm(mean,var**0.5).pdf(x)\n",
    "    return x,y\n",
    "\n",
    "# Plotting the same plots again, but this time we superimpose the normal curve to see the fit\n",
    "fig, ax = plot_coin_flip_CLT(flip_distribution, n, True)\n",
    "\n",
    "for i, dist in enumerate(flip_distribution):\n",
    "    x,y = get_normal_points(0.5*n[i],0.25*n[i],min(dist),max(dist))\n",
    "    # +0.5 is to offset, observe that for i heads it will be plotted between i and i+1 when there are only 3 bins\n",
    "    # Thus the normal curve should be plotted on i+0.5\n",
    "    # This is only an issue for n=3 case, when there are sufficient bins, the offset is negligible\n",
    "    ax[i].plot(x+0.5,y)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()    \n",
    "\n",
    "# Print the estimations\n",
    "print('Best estimates')\n",
    "for i, dist in enumerate(flip_distribution):\n",
    "    print(f'n={n[i]:<2}: μ=mean/n={np.mean(dist)/n[i]:.2f}, σ^2=var/n={np.var(dist)/n[i]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a close agreement between the best estimate and theoretical mean and variance.\n",
    "\n",
    "In the following section, we apply the same idea to a N-sided fair dice, though with a slight modification. For each sample, we take the **average** of the the rolls within that sample, this is in contrast to the coin flip example, where we took the **total** (i.e. *sum*) of the number of heads.\n",
    "* By taking the average, we restrict the possible sample values to the range $1$ to $n$, making it independent of sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_N_sided_dice(sides = 6, size = 30, samples = 2000):\n",
    "    # Theoretical mean and variance\n",
    "    mean_theory = np.mean(range(1,sides+1))\n",
    "    var_theory = np.var(range(1,sides+1))/size\n",
    "\n",
    "    # List of sample means (sample size as specified) of n-sided dice rolls\n",
    "    dice_mean_dist =  [np.mean(np.random.randint(1, sides+1, size)) for _ in range(samples)]\n",
    "    \n",
    "    # Getting points for normal graph\n",
    "    x,y = get_normal_points(mean_theory,var_theory,min(dice_mean_dist),max(dice_mean_dist))\n",
    "    \n",
    "    plt.hist(dice_mean_dist,density=True)\n",
    "\n",
    "    # Not doing the offset, low sample sizes should not be used, especially after CLT was just demonstrated\n",
    "    plt.plot(x,y)\n",
    "    plt.title(f'Distribution of means using {sides}-sided dice\\nSamples: {samples}, Sample size: {size}')\n",
    "    plt.ylabel('Relative frequency')\n",
    "    plt.show()\n",
    "\n",
    "plot_N_sided_dice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theoretical normal distribution is superimposed as a qualitative gauge for how well it fits. Feel free run the above cell with different number of sides, and various sample sizes, and check for yourself that the subsequent distribution tends towards normal when sample size is at least 30.\n",
    "\n",
    "### Limitations\n",
    "Nonetheless, CLT only works when the original distribution has a **well-defined (i.e. finite) mean and variance**. For all finite discrete probability distribution functions, such as dice roll or the coin flip, it is always possible to directly compute a finite mean and variance.\n",
    "* CLT works for all finite discrete probability distribution functions.\n",
    "\n",
    "However, this may not be true when moving into a continuous distribution function. Here, we draw samples from a pareto distribution, using a process similar to binomial, but with ```numpy.random.pareto``` instead. Much like how the binomial distribution is characterised by the probability $p$, the pareto distribution is characterised by the parameter $a$. Here are some known characteristics for various $a$ values.\n",
    "\n",
    "* When $a \\le 1$, the distribution has an infinite mean.\n",
    "* When $a \\le 2$, the distribution has an infinite variance.\n",
    "\n",
    "Hence, when $a \\le 2$, we expect CLT to *fail*, and when $a > 2$, we expect CLT to succeed. In the following section, we test this idea using $a=1,1.5,4$, and observe how it looks like when CLT fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual sample sizes to test\n",
    "n = [3, 15, 30, 50]\n",
    "# Indiviudal a values to test\n",
    "a = [1, 1.5, 5]\n",
    "\n",
    "# Generate 2000 independent samples\n",
    "s = 2000\n",
    "\n",
    "\n",
    "def pareto_mean(a, size):\n",
    "    return np.mean(np.random.pareto(a, size))\n",
    "\n",
    "\n",
    "def plot_pareto(result):\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=len(n), figsize=(4*len(n), 3))\n",
    "\n",
    "    for i, dist in enumerate(result):\n",
    "        # Plot histogram and customize\n",
    "        ax[i].hist(dist, bins=30, density=True)\n",
    "        ax[i].set_title(f'$n$ = {n[i]}')\n",
    "    ax[0].set_ylabel('Relative frequency')\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "# For each a, generate 2000 samples for each sample size\n",
    "for a_val in a:\n",
    "    print(f'a = {a_val}')\n",
    "    pareto_data = [[pareto_mean(a_val, size) for _ in range(s)] for size in n]\n",
    "    fig,ax = plot_pareto(pareto_data)\n",
    "    # Normal overlay using the sample statistics\n",
    "    for i, dist in enumerate(pareto_data):\n",
    "        x, y = get_normal_points(np.mean(dist), np.var(\n",
    "            dist), np.min(dist), np.max(dist))\n",
    "        ax[i].plot(x, y)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we can observe that when $a \\le 2$, the distribution of means does not converge to a normal distribution, regardless of the sample size.\n",
    "* Since for some $a$ values there is no defined mean and variance, the normal distribution overlay instead uses the *sample* mean and variance.\n",
    "\n",
    "For $a=5$, we observe a trend towards the normal distribution with increasing sample size, following the above discussions. Feel free to change the values of both $a$ and $n$ and observe how it changes the subsequent distribution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Demonstration\n",
    "### Random Walk\n",
    "\n",
    "Remember the coin flip example above? Flipping a fair coin is essentially the concept behind a **one-dimensional random walk**. By flipping a fair coin, we can decide the next step that an object at the origin along a linear axis moves. \n",
    "* At each step, the object can either move a distance of +1 (forward / *'head'*) or -1 (backwards / *'tail'*) with equal probability $p=0.5$. \n",
    "* When the object moves $n$ number of steps, assuming that the steps are independent and that $n$ is large enough, the probability distribution of the object's position approaches a Normal distribution, just like the coin example above!\n",
    "\n",
    "In this section, we demonstrate that for a random walk in N-dimensions (ND), the distribution of final displacements, at sufficient large number of steps, follow the normal distribution for **every single dimension** in accordance to the CLT.\n",
    "* We will be using ```np.random.choice``` to choose between $-1$ and $1$ to get the next position.\n",
    "* Note that simulating N-dimensional random walks is equivalent to simulating N number of independent 1D random walks, and inputting them into arrays for each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_pos(prev_pos,p=0.5):\n",
    "    return prev_pos + np.random.choice([-1, 1],p=[1-p,p])\n",
    "\n",
    "\n",
    "def random_walk_1D(steps=100):\n",
    "    pos_list = [0]\n",
    "    for _ in range(steps):\n",
    "        pos_list.append(next_pos(pos_list[-1]))\n",
    "    return np.array(pos_list)\n",
    "\n",
    "\n",
    "def random_walk_ND(steps=100, n=2):\n",
    "    # ND walk = N*1D walk -- independence across dimensions\n",
    "    # Access via arr[dim,time]\n",
    "    return np.array([random_walk_1D(steps) for _ in range(n)])\n",
    "\n",
    "\n",
    "def ND_walk_displacement(steps=100, n=2, samples=1000):\n",
    "    # Take specified samples, for each of them, take the position at last step of each dimension\n",
    "    # Access via arr[sample,dim]\n",
    "    return np.array([random_walk_ND(steps, n)[:, -1] for _ in range(samples)])\n",
    "\n",
    "\n",
    "# Data\n",
    "n = 3\n",
    "ND_walk_pos = ND_walk_displacement(n=n, samples=500)\n",
    "\n",
    "# Plot ND distribution\n",
    "fig, ax = plt.subplots(ncols=n, figsize=(4*n, 3))\n",
    "for i in range(n):\n",
    "    ax[i].hist(ND_walk_pos[:, i],density=True)\n",
    "    ax[i].set_title(f'Final pos in dimension {i+1}')\n",
    "\n",
    "# Customize and show\n",
    "ax[0].set_ylabel('Relative frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ```random_walk_ND``` function we derived above, we are able to simulate an animated 2D and 3D random walk. This visual representation shows each *discrete* step taken by the object being random in any dimension, thereby making it perhaps more intuitive to understand the following section, where we will explain how random walk is an approximation of Brownian motion, and conversely, Brownian motion is the limit of random walk!\n",
    "\n",
    "#### Animations of a 2D Random Walk\n",
    "To create the animation, we use the function ```FuncAnimation``` from the module ```matplotlib.animation```. After saving it as a gif, we load the gif using ```Image``` from ```Ipython.display```. The same process will be used in 3D random walk later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "# Creating a 2D random walk\n",
    "steps = 100\n",
    "walked_2D_pos = random_walk_ND(steps, n=2)\n",
    "# Split into x and y dimensions\n",
    "x, y = walked_2D_pos\n",
    "\n",
    "# Initialize plot\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# Place the origin for reference\n",
    "ax.scatter([0], [0], color='black', label='origin')\n",
    "\n",
    "# The main plot will change throughout animation, so we create a container for it.\n",
    "line1, = ax.plot([], [], '-r', label='path', lw=1)\n",
    "point1, = ax.plot([], [], 'or', label='point')\n",
    "\n",
    "# Customize\n",
    "ax.set_title('2D Random Walk')\n",
    "\n",
    "# Use the maximum displacement across x and y throughout the run to define our bounds for the plot\n",
    "bound = np.max(np.abs(walked_2D_pos))*1.1\n",
    "ax.set(xlim=(-bound, bound), ylim=(-bound, bound))\n",
    "\n",
    "# Animate Frame for 2D\n",
    "def animate_frame_2D(i):\n",
    "    '''\n",
    "    Defines what the i-th frame of animation should have.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    i: Specifies the frame of animation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List of plot elements that need to be updated\n",
    "    '''\n",
    "    # Line from 0 to i-th (inclusive) data point, and a point for the i-th data point\n",
    "    line1.set_data(x[:i+1], y[:i+1])\n",
    "    point1.set_data(x[i], y[i])\n",
    "\n",
    "    return line1, point1\n",
    "\n",
    "\n",
    "# blit=True optimizes the graphing process by only redrawing the changes.\n",
    "# It does so using the return values of animate.\n",
    "# Therefore, animate_frame returns the list of objects need updating.\n",
    "ani = FuncAnimation(fig, animate_frame_2D, blit=True, frames=steps)\n",
    "\n",
    "# In particular, gifs outputs are not rendered for .ipynb, so saving and redirecting to output should be used instead.\n",
    "ani.save(\"2D_walk.gif\", writer=PillowWriter(fps=25), dpi=200)\n",
    "# Suppress the static image\n",
    "plt.close()\n",
    "\n",
    "# Show saved gif\n",
    "Image(filename=\"2D_walk.gif\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Animation of a 3D Random Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "# Creating a 3D random walk\n",
    "steps = 100\n",
    "walked_3D_pos = random_walk_ND(steps, n=3)\n",
    "# Split into x,y,z dimensions\n",
    "x, y, z = walked_3D_pos\n",
    "\n",
    "# Initialize plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "\n",
    "# Place the origin for reference\n",
    "ax.scatter([0], [0], [0], color='black', label='origin')\n",
    "\n",
    "# The main plot will change throughout animation, so create a container for it\n",
    "line1, = ax.plot([], [], [], label='path', lw=1)\n",
    "\n",
    "# Customize\n",
    "ax.set_title('3D Random Walk')\n",
    "# Bounds\n",
    "# Use the maximum displacement across x, y and z throughout the run to define our bounds for the plot\n",
    "bound = np.max(np.abs(walked_3D_pos))*1.1\n",
    "ax.set(xlim3d=(-bound, bound), ylim3d=(-bound, bound), zlim3d=(-bound, bound))\n",
    "\n",
    "ax.set(xlabel='x', ylabel='y', zlabel='z')\n",
    "ax.set(xticklabels=[], yticklabels=[], zticklabels=[])\n",
    "\n",
    "# Animate Frame for 3D\n",
    "def animate_frame_3D(i):\n",
    "    '''\n",
    "    Defines what the i-th frame of animation should have.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    i: Specifies the frame of animation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List of plot elements that need to be updated\n",
    "    '''\n",
    "    # Line from 0 to i-th (inclusive) data point, and a point for the i-th data point\n",
    "    line1.set_data(x[:i+1], y[:i+1])\n",
    "    line1.set_3d_properties(z[:i+1])\n",
    "    return line1\n",
    "\n",
    "# blit=True optimizes the graphing process by only redrawing the changes.\n",
    "# It does so using the return values of animate.\n",
    "# Therefore, animate_frame returns the list of objects need updating.\n",
    "ani = FuncAnimation(fig, animate_frame_3D, frames=steps)\n",
    "\n",
    "# In particular, gifs outputs are not rendered for .ipynb, so saving and redirecting to output should be used instead.\n",
    "ani.save(\"3D_walk.gif\", writer=PillowWriter(fps=22), dpi=200)\n",
    "# Suppress the static image\n",
    "plt.close()\n",
    "\n",
    "# Show saved gif\n",
    "Image(filename=\"3D_walk.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brownian motion as a limit of random walk\n",
    "\n",
    "Following from our discussion about random walks as an application of CLT, we now discuss a similar concept - Brownian motion. Brownian motion is first observed by biologist Robert Brown under the microscope, where *pollen grains moved in random motion suspended in water*. Albert Einstein later reasoned that the observed Brownian motion is due to the macroscopic particle (pollen grain) undergoing collisions with microscopic atoms, resulting in \"random steps\". Mathematicians tried to construct a function to describe the Brownian motion, until Norbert Wiener came up with the rigorous mathematical model of Brownian motion that is called the **Wiener process**. \n",
    "\n",
    "In fact, the random walk is an approximation of Brownian motion, and the approximation becomes closer when the displacement of each step becomes smaller. In other words,\n",
    "> Brownian motion is a continuous-space and -time stochastic process, while a random walk is a discrete-space and -time model. \n",
    "\n",
    "Since real motion is continuous, we want to try to make random walk continuous by appropriate scaling:\n",
    "* Consider that in $t$ seconds, we take $n$ independent steps, each step has length $d$ $(p=0.5$ of $±d)$\n",
    "  * Equivalently, we take a step every $\\delta t = t/n$ seconds.\n",
    "  * n should be really large (>> 30)\n",
    "  * By CLT, **distribution of displacement after t seconds is normal** with (mean,var) = $(0, n d^2) = (0,(t/\\delta t) d^2)$\n",
    "    \n",
    "  * *Recall that:* \n",
    "    * mean = $0.5(d) + 0.5(-d) = 0$\n",
    "    * var (each) = $0.5*(-d)^2 + 0.5*(d)^2 = d^2$ -> $nd^2$ for $n$ steps\n",
    "* For convenience, we choose d = sqrt($\\delta t$).\n",
    "  * In particular, for the time interval $t$, the distribution is normal with (mean,var) = (0, t)\n",
    "  \n",
    "This is precisely the Wiener process, W(t) = N(0,t), **where the normality is ensured by CLT**. \n",
    "  * Here, the amount of diffusion depends on $t$ only, independent of past events\n",
    "  * The diffusion between any time interval between $t_1$ and $t_2$ is also normal, $W(t_2) - W(t_1) = N(0,(t_2-t_1))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiener_process_time(steps_per_second=100,duration=5):\n",
    "    # We need an additional array to map the i-th position entry to a time for plotting\n",
    "    return np.linspace(0,duration,endpoint=True,num=steps_per_second*duration+1)\n",
    "\n",
    "def single_wiener_process(steps_per_second=100,duration=5):\n",
    "    # It's just a scaled random walk\n",
    "    return random_walk_1D(steps_per_second*duration)/steps_per_second**0.5\n",
    "\n",
    "def multi_wiener_process(steps_per_second=100,duration=5,runs=500):\n",
    "    return np.array([single_wiener_process(steps_per_second,duration) for _ in range(runs)],dtype=object)\n",
    "\n",
    "def plot_wiener_process(time, result):\n",
    "    fig, ax = plt.subplots(ncols =2, figsize=(8,4))\n",
    "    \n",
    "    # Plot runs\n",
    "    for run in result:\n",
    "        ax[0].plot(time,run)\n",
    "    \n",
    "    final_pos = result[:,-1] \n",
    "\n",
    "    # Plot distribution of final position\n",
    "    ax[1].hist(final_pos)\n",
    "\n",
    "    # Customize\n",
    "    ax[0].set_ylabel('Position')\n",
    "    ax[1].set_ylabel('Count')\n",
    "    \n",
    "    ax[0].set_title(f'Visualization of all {len(result)} runs')\n",
    "    ax[1].set_title(f'Distribution of final positions\\n$\\mu$ = {np.mean(final_pos):.2f}, $\\sigma^2$ = {np.var(final_pos):.2f}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for d in [3,5,10]:\n",
    "    print(f'Duration = {d} s')\n",
    "    print(f'Expected distribution: N(0,{d})')\n",
    "    plot_wiener_process(wiener_process_time(duration=d),multi_wiener_process(duration=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gambler's Ruin Problem\n",
    "\n",
    "Consider a gambler who starts with \\$i. \n",
    "\n",
    "On each successive gamble, he either wins \\\\$ 1 with a probability of $p$, or loses \\\\$1 with a probability of $(1−p)$, independent of the past.\n",
    "\n",
    "The game ends when:\n",
    "- The gambler earns his desired amount of $\\$N$ ($0 < i < n$) and wins, or\n",
    "- The gambler goes broke, i.e., is ruined,\n",
    "\n",
    "whichever happens first.\n",
    "\n",
    "Let us define some terms:\n",
    "- $R_n$ - Total fortune after the $n^{th}$ gamble. (0 < i < N)\n",
    "- $P_i$ - Probability that the gambler wins when $R_0 = i$.\n",
    "- $\\mu_i$ - Expected number of plays until the game ends, starting from $i$\n",
    "\n",
    "We can explore this scenario by modelling the system with random walks of step length step length +1 (with probability $p$) or -1 (with a propability $(1-p)$). For each case, we will simulate a specified number of runs and plot the distributions of the outcomes.  \n",
    "\n",
    "**Case 1: Fair Game ($p = 0.5$)**\n",
    "\n",
    "It can be proven that:\n",
    "- $P_i = \\dfrac{i}{N}$\n",
    "- $\\mu_i = i(N-i)$\n",
    "\n",
    "**Case 2: Unfavourable Odds ($p\\neq q$)**\n",
    "\n",
    "It can be proven that: ($r=\\dfrac{1-p}{p}$)\n",
    "- $P_i = \\dfrac{1-r^i}{1-r^N}$\n",
    "- $\\mu_i = \\dfrac{r+1}{r-1}(i-\\dfrac{N(1-r^i)}{1-r^N})$\n",
    "\n",
    "We can also prove that, if a player plays until he goes broke (i.e., as $N$ approaches infinity):\n",
    "- If $p>0.5$, there is a nonzero probability that the player will become infinitely rich.\n",
    "- If $p\\le0.5$, the player will definitely go broke. In other words, a gambler playing a game with negative expected value will eventually be ruined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_gambler_ruin(i=10,N=20,p=0.5):\n",
    "    fortune_list = [i]\n",
    "    while True:\n",
    "        # Reusing random walk code!\n",
    "        fortune_list.append(next_pos(fortune_list[-1],p))\n",
    "        if fortune_list[-1] == 0 or fortune_list[-1] == N:\n",
    "            return np.array(fortune_list)\n",
    "\n",
    "def multi_gambler_ruin(i=10,N=20,p=0.5,runs=100):\n",
    "    # Access via pot_n = arr[run,n]\n",
    "    return np.array([single_gambler_ruin(i,N,p) for _ in range(runs)],dtype=object)\n",
    "\n",
    "def plot_gambler_ruin(result):\n",
    "    fig, ax = plt.subplots(ncols =3, figsize=(12,4))\n",
    "    \n",
    "    # Plot runs, and also count number of losses and wins\n",
    "    loss, win = 0,0\n",
    "    for run in result:\n",
    "        ax[0].plot(run)\n",
    "        if run[-1] == 0:\n",
    "            loss += 1\n",
    "        else:\n",
    "            win += 1\n",
    "    \n",
    "    # Plot time taken\n",
    "    run_lengths = [len(run) for run in result]\n",
    "    ax[1].hist(run_lengths)\n",
    "\n",
    "    # Plot wins vs loss\n",
    "    ax[2].bar((\"Won\",\"Ruined\"),(win, loss), width = 0.5)\n",
    "    \n",
    "    # Customize\n",
    "    ax[0].set_ylabel('Money')\n",
    "    ax[1].set_ylabel('Counts')\n",
    "    ax[2].set_ylabel('Counts')\n",
    "\n",
    "    ax[0].set_title(f'Visualization of all {len(result)} runs')\n",
    "    ax[1].set_title(f'Distribution of gambles\\n$\\mu$ = {np.mean(run_lengths):.2f}, $\\sigma^2$ = {np.var(run_lengths):.2f}')\n",
    "    ax[2].set_title(f'Success rate: {win/(loss+win):.2f}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_gambler_ruin(multi_gambler_ruin(p=0.7,runs=100))\n",
    "\n",
    "# TODO: Think if this is really adds value -- connect to CLT..?\n",
    "#     if p == 0.5:\n",
    "#         print(\"Expected probability of wins = \", i/N)\n",
    "#         print(\"Expected number of plays = \", i*(N-i))\n",
    "    \n",
    "#     else:\n",
    "#         r = (1-p)/p\n",
    "#         print(\"Expected probability of wins = \", (1-r**i)/(1-r**N))\n",
    "#         print(\"Expected number of plays = \", ((r+1)/(r-1))*(i-(N*(1-r**i))/(1-r**N)))\n",
    "\n",
    "#     print(\"Probability of wins in simulation = \", number_of_wins/number_of_runs)\n",
    "#     print(\"Average number of plays in simulation = \", np.average(number_of_plays))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_gambler_ruin_infinite(i=10,p=0.5,max_step=1000):\n",
    "    # Highly similar to finite version, except modified exit condition\n",
    "    fortune_list = [i]\n",
    "    while len(fortune_list) < max_step:\n",
    "        # Reusing random walk code!\n",
    "        fortune_list.append(next_pos(fortune_list[-1],p))\n",
    "        if fortune_list[-1] == 0:\n",
    "            break\n",
    "    return np.array(fortune_list)\n",
    "\n",
    "def gamblers_ruin_infinite(i=50, p=0.7, runs=100, max_step=1000):\n",
    "    return np.array([single_gambler_ruin_infinite(i,p,max_step) for _ in range(runs)],dtype=object)\n",
    "\n",
    "for p in [0.54,0.52,0.5,0.48,0.46]:\n",
    "    print(f'p={p}')\n",
    "    plot_gambler_ruin(gamblers_ruin_infinite(p=p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading6b.pdf\n",
    "2. http://astro.pas.rochester.edu/~aquillen/phy256/lectures/Diffusion_walks.pdf\n",
    "3. https://www.stat.cmu.edu/~cshalizi/754/notes/lecture-16.pdf\n",
    "4. https://towardsdatascience.com/central-limit-theorem-a-real-life-application-f638657686e1\n",
    "5. https://web.stanford.edu/class/archive/cs/cs109/cs109.1214/lectures/18-CentralLimitTheorem/18-CentralLimitTheorem.pdf\n",
    "6. https://www.probabilitycourse.com/chapter11/11_4_1_brownian_motion_as_the_limit_of_a_symmetric_random_walk.php\n",
    "7. https://cims.nyu.edu/~holmes/teaching/asa19/handout_Lecture6_2019.pdf\n",
    "8. https://galton.uchicago.edu/~lalley/Courses/313/BrownianMotionCurrent.pdf\n",
    "9. http://www.columbia.edu/~ks20/FE-Notes/4700-07-Notes-GR.pdf\n",
    "10. https://www.youtube.com/watch?v=Ne2lmAZI4-I\n",
    "11. https://web.mit.edu/neboat/Public/6.042/randomwalks.pdf\n",
    "12. https://www.academia.edu/18573894/Variance_of_the_game_duration_in_the_gambler_s_ruin_problem "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e0d091042142f08f95fbe24eea5f30d9f84a0503ed0a5ab9d4f63f212faef8cd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
