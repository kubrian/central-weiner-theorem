{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of Central Limit Theorem\n",
    "Michelle Yaochai | Zheng Chong, Emily | Marvin Dragon Choo | Brian Ku\\\n",
    "01 Apr 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "We explore the central limit theorem (CLT) and their applications in this notebook.\n",
    "\n",
    "* In Part A, we define and illustrate CLT using examples such as coin flips and rolling a fair dice. We also show how to use CLT to calculate population parameters from samples, as well as situations where CLT fails.\n",
    "* In Part B, we demonstrate the utility of CLT by simulating various physical systems, and validating key parameters against those predicted by CLT, and use it to calculate physical properties such as diffusion coefficients, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following modules will be used in this notebook, each of which will be introduced as they are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Illustration\n",
    "\n",
    "### Motivation (TODO)\n",
    "first of all, def need to add where clt is used before definition. like clt is a concept found in statistics/statistical analysis when we have a population (defined) and we are trying to measure a characteristic of it. For example, in the population of CHS students, we want to see what the distribution of GPA? /height is like. Thus, we carry out a probability sample. \n",
    "\n",
    "...... then the fact that if you take enough probability samples and plot the sum of results in each sample, the distribution would always look sth like this [insert perfect histogram], where the mean, median and mode are in the center. this is a normal distribution and .... segue into clt definition\n",
    "\n",
    "### Definition\n",
    "Here, we give a common definition of Central Limit Theorem (CLT):\n",
    "\n",
    "> The distribution of sample means approximates a normal distribution as the sample size gets larger, even if the population is not normally distributed.\n",
    "\n",
    "This definition is certainly not the most mathematically rigorous, but is sufficient for the scope of the project. There are also additional restrictions necessary for CLT, and they will be introduced as appropriate.\n",
    "\n",
    "We now illustrate the assertion that CLT is trying to make using coin flips. Consider the act of flipping a fair coin.\n",
    "* Since the coin is fair, there is an equal probability $p=0.5$ of it landing on heads or tails.\n",
    "* Assign the outcome of tails as 0, and heads as 1.\n",
    "\n",
    "This system is equivalently represented by a binomial distribution (since there are only two outcomes) with probability of success (of getting heads) of 0.5.\n",
    "\n",
    "To begin, let's see what happens when we flip the coin 100 times. Since this is a binomial distribution, we can make use of ```numpy.random.binomial``` to simulate these coin flips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "p = 0.5\n",
    "\n",
    "# Labels\n",
    "flip_outcomes = ['Tail', 'Head']\n",
    "\n",
    "# Data\n",
    "# Count the number of heads after flipping a coin n times, each with probability p of landing on heads.\n",
    "num_heads = np.random.binomial(n=n,p=p)\n",
    "# \n",
    "num_tails = n - num_heads\n",
    "\n",
    "# Show outcome\n",
    "print(f'Out of {n} coin flips, there were {num_heads} heads and {num_tails} tails.')\n",
    "\n",
    "# Plot\n",
    "# Expected\n",
    "plt.bar(flip_outcomes, [num_tails/n, num_heads/n], width=0.25, label='Actual')\n",
    "# Actual\n",
    "plt.hlines(y=0.5, xmin=-0.6, xmax=1.6, linestyles='--',\n",
    "           colors='red', label='Expected')\n",
    "\n",
    "# Customize and Show\n",
    "plt.xlim([-0.5, 1.5])\n",
    "plt.title(f'Outcome of {n} dice flips')\n",
    "plt.legend(loc='lower center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If asked to predict the number of heads, it is probably *instinctive* to expect 50 of each, after all, we expect an equal number of heads and tails.\n",
    "\n",
    "Most of the time, we get a number that *feels* close to, but not exactly 50. Try running the above cell multiple times! Here, we take this opportunity to explain more key terms.\n",
    "\n",
    "* Each time you execute the cell, you generate one **sample** of 100 coin flips.\n",
    "* The **sample size $n$** is the number of independent coin flips that were performed. Here, it is 100.\n",
    "* The direct output of each sample, which is a number between 0 and 100, is the total number of heads in that sample.\n",
    "\n",
    "### Sampling Distribution\n",
    "Now that you've run the cell multiple times, you can see that each time you run it, you get a different result. Even though the coin is the same, there is still some variability between the different samples collected from this 'population'.\n",
    "\n",
    "Yet, it is not equally likely to obtain each outcome. Using a sample size of 100, it *feels* more probable to observe a sample with 40 heads, than that of 10 heads, which feels more extreme. We can visualize this probability by simulating many independent samples, and observing the distribution of the number of heads, using a histogram.\n",
    "* This is what *'distribution of sample means'* in the definition of CLT is referring to.\n",
    "\n",
    "Surprisingly, generating this data can be done in one line of code, again using ```numpy.random.binomial```. In particular, we vary the sample sizes and observe the impact on the subsequent distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual sample sizes to test\n",
    "n = [3, 15, 30, 50]\n",
    "# Fair coin, equal chance of heads and tails\n",
    "p = 0.5\n",
    "# Generate 2000 independent samples\n",
    "s = 2000\n",
    "\n",
    "# Data\n",
    "# Access via dist = arr[sample_size_index], dist = [mean1, mean2, ... mean2000]\n",
    "flip_distribution = [np.random.binomial(i, p, s) for i in n]\n",
    "\n",
    "# We will reuse this later, so putting it in a function first\n",
    "\n",
    "\n",
    "def plot_coin_flip_CLT(flip_distribution, n, normalize = False):\n",
    "    fig, ax = plt.subplots(ncols=len(n), figsize=(3*len(n), 4))\n",
    "\n",
    "    # Making good histograms\n",
    "    for i, dist in enumerate(flip_distribution):\n",
    "        min_value = min(dist)\n",
    "        max_value = max(dist)\n",
    "\n",
    "        # Max ensures that step sizes in the following histogram are at least 1, but otherwise such that there will be around 30 bins\n",
    "        step_size = max((max_value-min_value)//30, 1)\n",
    "        # n+2 ensures the outcome with all heads is also captured.\n",
    "        bin_range = range(0, n[i]+2, step_size)\n",
    "\n",
    "        # Plot histogram and customize\n",
    "        ax[i].hist(dist, bins=bin_range, density=normalize)\n",
    "        ax[i].set_xlabel('Heads')\n",
    "        ax[i].set_ylabel('Counts')\n",
    "        ax[i].set_title(f'$n$ = {n[i]}')\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "plot_coin_flip_CLT(flip_distribution, n)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restating CLT from above, we can quite clearly observe that:\n",
    "> The *distribution of sample means* approximates a normal distribution *as the sample size gets larger*, even if the population is not normally distributed.\n",
    "\n",
    "* In particular, the 'distribution of population' here is the distribution of the probability of the outcomes of coin flip - 0.5 for heads and tails, certainly not normally distributed.\n",
    "\n",
    "### CLT conditions\n",
    "In practice, we impose some further conditions before using CLT, to account for more extreme probability distribution functions. We will summarise the conditions necessary for CLT to hold true:\n",
    "\n",
    "* The data must be sampled randomly from the population by using a probability sampling method.\n",
    "* Each of the sample values must be independent of each other. That is, the occurrence of one event does not affect the occurrence of any other event.\n",
    "* When the sample is drawn without replacement, the sample size must be no larger than 10% of the total population.\n",
    "* Sample sizes equal to or greater than 30 are often considered sufficient for the CLT to hold.\n",
    "\n",
    "If all these conditions are fulfilled, then, by the central limit theorem, the sampling distribution obtained will represent a normal distribution, regardless whether the variable actually follows a normal distribution in the population. (eg. In Asia, people's heights are skewed towards the shorter end.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Application\n",
    "We can now easily control the conditions of sampling to generate a normal distribution. To turn CLT into a useful tool, we need to establish a link between the sample statistics and population statistics.\n",
    "\n",
    "### Population statistics\n",
    "The mean and variance of a single flip of a fair coin is given by:\n",
    "$$\\mu = 0.5(0) + 0.5(1) = 0.5$$\n",
    "$$\\sigma^2 = 0.5 (1-0.5)^2 + 0.5 (0-0.5)^2 = 0.25$$\n",
    "\n",
    "### Expectation value\n",
    "Scaling this up to $n$ independent coin flips, we *expect* the subseuqent distribution to have both mean and variance scaled up by $n$.\n",
    "* The mathematical derivation is beyond the scope of this project.\n",
    "\n",
    "Let $X$ represent the distribution of number of heads (i.e. what we have been plotting), then,\n",
    "$$E(\\bar{X}) = n\\mu$$\n",
    "where $\\bar{X}$ is mean number of heads, and $\\mu$ is the expected number of heads for a flip.\n",
    "$$E(Var(X))=n\\sigma^2$$\n",
    "where $Var(X)$ is the variance of the number of heads, and $\\sigma^2$ is the expected variance in the number of heads per flip.\n",
    "\n",
    "### Utility\n",
    "This also means that if we did not know the population mean and variance (for example, if we **did not know** that the coin is fair, and this is true in the applications of CLT), our best guess for them would then be\n",
    "$$\\mu=\\bar{X}/n$$\n",
    "$$\\sigma^2 = Var(X)/n$$\n",
    "\n",
    "Therefore, we do the following here:\n",
    " \n",
    "1. Compare the *expected* distribution against the actual data, and show a good agreement for high $n$ values.\n",
    "   * The expected distribution is normal, with $(\\mu,\\sigma^2)=(0.5n,0.25n)$\n",
    "   * In a normal curve, a higher mean corresponds to a curve shifted to the right, and a higher variance corresponds to a wider curve.\n",
    "   * We use ```scipy.stats.norm``` to obtain this normal curve.\n",
    "2. Estimate the true mean and variance of the coin from the observed distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normal_points(mean=0,var=1,x_min=0,x_max=1):\n",
    "    x = np.linspace(x_min,x_max,endpoint=True)\n",
    "    # norm takes in mean,std\n",
    "    y = norm(mean,var**0.5).pdf(x)\n",
    "    return x,y\n",
    "\n",
    "# Plot again, but this time we superimpose the normal curve to see the fit\n",
    "fig, ax = plot_coin_flip_CLT(flip_distribution, n, True)\n",
    "\n",
    "for i, dist in enumerate(flip_distribution):\n",
    "    x,y = get_normal_points(0.5*n[i],0.25*n[i],min(dist),max(dist))\n",
    "    # +0.5 is to offset, observe that for i heads it will be plotted between i and i+1 when there are only 3 bins\n",
    "    # Thus the normal curve should be plotted on i+0.5\n",
    "    # This is only an issue for n=3 case, when there are sufficient bins, the offset is negligible\n",
    "    ax[i].plot(x+0.5,y)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()    \n",
    "\n",
    "# Print the estimations\n",
    "print('Best estimates')\n",
    "for i, dist in enumerate(flip_distribution):\n",
    "    print(f'n={n[i]}: μ=mean/n={(np.mean(dist)/n[i]).round(3)}, σ^2=var/n={(np.var(dist)/n[i]).round(3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random dice roll, N sided fair dice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_N_sided_dice(sides = 6, size = 30, samples = 1000):\n",
    "    # Theoretical mean and variance\n",
    "    mean_theory = np.mean(range(1,sides+1))\n",
    "    var_theory = np.var(range(1,sides+1))\n",
    "\n",
    "    # List of sample means (sample size as specified) of n-sided dice rolls\n",
    "    dice_mean_dist =  [np.mean(np.random.randint(1, sides+1, size)) for _ in range(samples)]\n",
    "    \n",
    "    # Getting points for normal graph\n",
    "    x,y = get_normal_points(mean_theory,var_theory/size,min(dice_mean_dist),max(dice_mean_dist))\n",
    "    \n",
    "    plt.hist(dice_mean_dist,density=True)\n",
    "\n",
    "    # Not doing the offset, low sample sizes should not be used, especially after CLT was just demonstrated\n",
    "    plt.plot(x,y)\n",
    "    plt.title(f'Distribution of means using {sides}-sided dice\\nSamples: {samples}, Sample size: {size}')\n",
    "    plt.show()\n",
    "\n",
    "plot_N_sided_dice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonetheless, CLT only works when the original distribution has a well defined mean and variance.\n",
    "* By direct computation, this is possible for all (non-infinite) discrete probability distribution functions, such as dice roll or the coin flip\n",
    "* However, this may not be true when moving into a continuous distribution function. Here, we draw samples from a pareto distribution, using a process similar to binomial, but with ```numpy.random.pareto``` instead. This distribution has an infinite mean and variance.\n",
    "* We note that no matter the sample size, the distribution of means does not even remotely converge to a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual sample sizes to test\n",
    "n = [3,15,30,50]\n",
    "# Generate 100 independent samples\n",
    "s = 100\n",
    "\n",
    "def pareto_mean(size):\n",
    "    # When a = 1, the distribution known to have infinite mean and variance\n",
    "    return np.mean(np.random.pareto(a=1,size=size))\n",
    "\n",
    "# For each sample size, generate 2000 samples\n",
    "pareto_data = [[pareto_mean(size) for _ in range(s)] for size in n]\n",
    "\n",
    "fig, ax = plt.subplots(ncols=len(n), figsize=(3*len(n), 4))\n",
    "\n",
    "for i, dist in enumerate(pareto_data):\n",
    "    # Plot histogram and customize\n",
    "    ax[i].hist(dist)\n",
    "    ax[i].set_ylabel('Counts')\n",
    "    ax[i].set_title(f'$n$ = {n[i]}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Walk\n",
    "1D Random Walk\n",
    "* In a simple one-dimentional random walk, an object at the origin along a linear axis can either move a distance of +1 or -1 with equal probability. This will be called a step. When the object moves N number of steps, assuming that the steps are independent N is large enough, the probability distribution of the object's position approaches a Normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_pos(prev_pos,p=0.5):\n",
    "    return prev_pos + np.random.choice([-1, 1],p=[1-p,p])\n",
    "\n",
    "\n",
    "def random_walk_1D(steps=100):\n",
    "    pos_list = [0]\n",
    "    for _ in range(steps):\n",
    "        pos_list.append(next_pos(pos_list[-1]))\n",
    "    return np.array(pos_list)\n",
    "\n",
    "\n",
    "def random_walk_ND(steps=100, n=2):\n",
    "    # ND walk = N*1D walk -- independence across dimensions\n",
    "    # Access via arr[dim,time]\n",
    "    return np.array([random_walk_1D(steps) for _ in range(n)])\n",
    "\n",
    "\n",
    "def ND_walk_displacement(steps=100, n=2, samples=1000):\n",
    "    # Take specified samples, for each of them, take the position at last step of each dimension\n",
    "    # Access via arr[sample,dim]\n",
    "    return np.array([random_walk_ND(steps, n)[:, -1] for _ in range(samples)])\n",
    "\n",
    "\n",
    "# Data\n",
    "n = 3\n",
    "ND_walk_pos = ND_walk_displacement(n=n, samples=500)\n",
    "\n",
    "# Plot ND distribution\n",
    "fig, ax = plt.subplots(ncols=n, figsize=(4*n, 3))\n",
    "for i in range(n):\n",
    "    ax[i].hist(ND_walk_pos[:, i])\n",
    "    ax[i].set_title(f'Final pos in dimension {i}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brownian motion as a limit of random walk\n",
    "\n",
    "* Scope: 1D brownian motion to establish theory, then apply to ND\n",
    "* Motvation: Real motion is continuous, thus we want to try to make random walk continuous by appropriate scaling\n",
    "* Consider in t seconds, we take n independent steps, each step has length d (50% +-d)\n",
    "  * Equivalently, we take a step every $\\delta t = t/n$ seconds.\n",
    "  * n should be really large (clearly >> 30), **by CLT, distribution of displacement after t seconds is normal with (mean,var) = $(0, n d^2) = (0,(t/\\delta t) d^2)$**\n",
    "    * recall\n",
    "    * mean = $0.5(d) + 0.5(-d) = 0$\n",
    "    * var (each) = $0.5*(-d)^2 + 0.5*(d)^2 = d^2$ -> $nd^2$ for $n$ steps\n",
    "* For convenience, we choose d = sqrt($\\delta t$).\n",
    "  * In particular, for the time interval t, the distribution is normal with (mean,var) = (0, t)\n",
    "* This is precisely the Wiener process W(t) = N(0,t), **where the normality is ensured by CLT**. Remarks\n",
    "  * The amount of diffusion depends on t only, independent of past events\n",
    "  * Diffusion between any time interval between t1 and t2 is also normal, W(t_2) - W(t_1) = N(0,(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiener_process_time(steps_per_second=100,duration=5):\n",
    "    # We need an additional array to map the i-th position entry to a time for plotting\n",
    "    return np.linspace(0,duration,endpoint=True,num=steps_per_second*duration+1)\n",
    "\n",
    "def single_wiener_process(steps_per_second=100,duration=5):\n",
    "    # It's just a scaled random walk\n",
    "    return random_walk_1D(steps_per_second*duration)/steps_per_second**0.5\n",
    "\n",
    "def multi_wiener_process(steps_per_second=100,duration=5,runs=500):\n",
    "    return np.array([single_wiener_process(steps_per_second,duration) for _ in range(runs)],dtype=object)\n",
    "\n",
    "def plot_wiener_process(time, result):\n",
    "    fig, ax = plt.subplots(ncols =2, figsize=(8,4))\n",
    "    \n",
    "    # Plot runs\n",
    "    for run in result:\n",
    "        ax[0].plot(time,run)\n",
    "    \n",
    "    final_pos = result[:,-1] \n",
    "\n",
    "    # Plot distribution of final position\n",
    "    ax[1].hist(final_pos)\n",
    "\n",
    "    # Customize\n",
    "    ax[0].set_ylabel('Position')\n",
    "    ax[1].set_ylabel('Count')\n",
    "    \n",
    "    ax[0].set_title(f'Visualization of all {len(result)} runs')\n",
    "    ax[1].set_title(f'Distribution of final positions\\n$\\mu$ = {np.mean(final_pos):.2f}, $\\sigma^2$ = {np.var(final_pos):.2f}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for d in [3,5,10]:\n",
    "    print(f'Duration = {d} s')\n",
    "    print(f'Expected distribution: N(0,{d})')\n",
    "    plot_wiener_process(wiener_process_time(duration=d),multi_wiener_process(duration=d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gambler's Ruin Problem\n",
    "\n",
    "Consider a gambler who starts with \\$i. On each successive gamble, he either wins \\$1 with a probability of $p$, or loses \\$1 with a probability of $(1−p)$, independent of the past.\n",
    "\n",
    "The game ends when:\n",
    "- The gambler earns his desired amount of $\\$N$ ($0 < i < n$) and wins, or\n",
    "- The gambler goes broke, i.e., is ruined,\n",
    "\n",
    "whichever happens first.\n",
    "\n",
    "Let us define some terms:\n",
    "- $R_n$ - Total fortune after the $n^{th}$ gamble. (0 < i < N)\n",
    "- $P_i$ - Probability that the gambler wins when $R_0 = i$.\n",
    "- $\\mu_i$ - Expected number of plays until the game ends, starting from $i$\n",
    "\n",
    "We can explore this scenario by modelling the system with random walks of step length step length +1 (with probability $p$) or -1 (with a propability $(1-p)$). For each case, we will simulate a specified number of runs and plot the distributions of the outcomes.  \n",
    "\n",
    "**Case 1: Fair Game ($p = 0.5$)**\n",
    "\n",
    "It can be proven that:\n",
    "- $P_i = \\dfrac{i}{N}$\n",
    "- $\\mu_i = i(N-i)$\n",
    "\n",
    "**Case 2: Unfavourable Odds ($p\\neq q$)**\n",
    "\n",
    "It can be proven that: ($r=\\dfrac{1-p}{p}$)\n",
    "- $P_i = \\dfrac{1-r^i}{1-r^N}$\n",
    "- $\\mu_i = \\dfrac{r+1}{r-1}(i-\\dfrac{N(1-r^i)}{1-r^N})$\n",
    "\n",
    "We can also prove that, if a player plays until he goes broke (i.e., as $N$ approaches infinity):\n",
    "- If $p>0.5$, there is a nonzero probability that the player will become infinitely rich.\n",
    "- If $p\\le0.5$, the player will definitely go broke. In other words, a gambler playing a game with negative expected value will eventually be ruined.\n",
    "\n",
    "**Sources:**\n",
    "\n",
    "- http://www.columbia.edu/~ks20/FE-Notes/4700-07-Notes-GR.pdf\n",
    "- https://www.youtube.com/watch?v=Ne2lmAZI4-I\n",
    "- https://web.mit.edu/neboat/Public/6.042/randomwalks.pdf\n",
    "- https://www.academia.edu/18573894/Variance_of_the_game_duration_in_the_gambler_s_ruin_problem (This gives the variance for the number of plays before game ends, which we will not use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_gambler_ruin(i=10,N=20,p=0.5):\n",
    "    fortune_list = [i]\n",
    "    while True:\n",
    "        # Reusing random walk code!\n",
    "        fortune_list.append(next_pos(fortune_list[-1],p))\n",
    "        if fortune_list[-1] == 0 or fortune_list[-1] == N:\n",
    "            return np.array(fortune_list)\n",
    "\n",
    "def multi_gambler_ruin(i=10,N=20,p=0.5,runs=100):\n",
    "    # Access via pot_n = arr[run,n]\n",
    "    return np.array([single_gambler_ruin(i,N,p) for _ in range(runs)],dtype=object)\n",
    "\n",
    "def plot_gambler_ruin(result):\n",
    "    fig, ax = plt.subplots(ncols =3, figsize=(12,4))\n",
    "    \n",
    "    # Plot runs, and also count number of losses and wins\n",
    "    loss, win = 0,0\n",
    "    for run in result:\n",
    "        ax[0].plot(run)\n",
    "        if run[-1] == 0:\n",
    "            loss += 1\n",
    "        else:\n",
    "            win += 1\n",
    "    \n",
    "    # Plot time taken\n",
    "    run_lengths = [len(run) for run in result]\n",
    "    ax[1].hist(run_lengths)\n",
    "\n",
    "    # Plot wins vs loss\n",
    "    ax[2].bar((\"Won\",\"Ruined\"),(win, loss), width = 0.5)\n",
    "    \n",
    "    # Customize\n",
    "    ax[0].set_ylabel('Money')\n",
    "    ax[1].set_ylabel('Counts')\n",
    "    ax[2].set_ylabel('Counts')\n",
    "\n",
    "    ax[0].set_title(f'Visualization of all {len(result)} runs')\n",
    "    ax[1].set_title(f'Distribution of gambles\\n$\\mu$ = {np.mean(run_lengths):.2f}, $\\sigma^2$ = {np.var(run_lengths):.2f}')\n",
    "    ax[2].set_title(f'Success rate: {win/(loss+win):.2f}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_gambler_ruin(multi_gambler_ruin(p=0.7,runs=100))\n",
    "\n",
    "# TODO: Think if this is really adds value -- connect to CLT..?\n",
    "#     if p == 0.5:\n",
    "#         print(\"Expected probability of wins = \", i/N)\n",
    "#         print(\"Expected number of plays = \", i*(N-i))\n",
    "    \n",
    "#     else:\n",
    "#         r = (1-p)/p\n",
    "#         print(\"Expected probability of wins = \", (1-r**i)/(1-r**N))\n",
    "#         print(\"Expected number of plays = \", ((r+1)/(r-1))*(i-(N*(1-r**i))/(1-r**N)))\n",
    "\n",
    "#     print(\"Probability of wins in simulation = \", number_of_wins/number_of_runs)\n",
    "#     print(\"Average number of plays in simulation = \", np.average(number_of_plays))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_gambler_ruin_infinite(i=10,p=0.5,max_step=1000):\n",
    "    # Highly similar to finite version, except modified exit condition\n",
    "    fortune_list = [i]\n",
    "    while len(fortune_list) < max_step:\n",
    "        # Reusing random walk code!\n",
    "        fortune_list.append(next_pos(fortune_list[-1],p))\n",
    "        if fortune_list[-1] == 0:\n",
    "            break\n",
    "    return np.array(fortune_list)\n",
    "\n",
    "def gamblers_ruin_infinite(i=50, p=0.7, runs=100, max_step=1000):\n",
    "    return np.array([single_gambler_ruin_infinite(i,p,max_step) for _ in range(runs)],dtype=object)\n",
    "\n",
    "for p in [0.54,0.52,0.5,0.48,0.46]:\n",
    "    print(f'p={p}')\n",
    "    plot_gambler_ruin(gamblers_ruin_infinite(p=p))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e0d091042142f08f95fbe24eea5f30d9f84a0503ed0a5ab9d4f63f212faef8cd"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
